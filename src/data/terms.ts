
import type { Term } from '@/lib/types';

function generateId(name: string): string {
  return name
    .toLowerCase()
    .replace(/\s+/g, '-')
    .replace(/[^\w-]+/g, '');
}

export const terms: Term[] = [
  // Foundational Concepts
  {
    id: generateId('Artificial Intelligence (AI)'),
    name: 'Artificial Intelligence (AI)',
    category: 'Foundational Concepts',
    content: {
      simpleDefinition: 'The broad field of computer science dedicated to creating machines that can perform tasks typically requiring human intelligence.',
      analogy: 'Think of AI as the entire field of "smart machines" – from simple calculators that can "think" through math problems to complex robots that can navigate and interact with the world.',
      whyItMatters: 'AI is the overarching domain that encompasses all the other terms here, representing the ambition to build intelligent systems that can augment or automate human capabilities.',
    },
  },
  {
    id: generateId('Generative AI'),
    name: 'Generative AI',
    category: 'Foundational Concepts',
    content: {
      simpleDefinition: 'AI that *creates content* by predicting the next token (text, images, music, code), rather than just analyzing existing data.',
      analogy: 'An artist or writer who *produces* new works, unlike an AI that just identifies existing ones.',
      example: 'An AI writing a new marketing email from bullet points.',
      elaboration: 'Unlike traditional (discriminative) AI that classifies or predicts based on existing patterns, generative AI learns to understand the underlying structure of data and then produces novel, original outputs that resemble the training data but are not identical copies. This capability extends to various modalities, from generating realistic human faces to composing musical pieces or even designing new molecules.',
      whyItMatters: 'Enables AI creativity, generating new content from sales copy to product designs, and opening up possibilities for automation in creative and knowledge-intensive tasks.',
    },
  },
  {
    id: generateId('Large Language Model (LLM)'),
    name: 'Large Language Model (LLM)',
    category: 'Foundational Concepts',
    content: {
      simpleDefinition: 'A powerful AI trained on vast text data to understand, generate, and respond to human language naturally.',
      analogy: 'A super well-read librarian who can also write essays and converse intelligently.',
      example: 'AI chatbots like ChatGPT or Gemini.',
      elaboration: 'These models are "large" because they contain billions or even trillions of model parameters, allowing them to capture complex linguistic patterns and world knowledge from the massive datasets they are trained on. This scale enables them to perform a wide range of natural language tasks, including translation, summarization, question-answering, and creative writing, often exhibiting emergent capabilities not explicitly programmed.',
      whyItMatters: 'LLMs are the foundation of most text-based generative AI, transforming information interaction and communication automation, and serving as versatile tools for various business and creative applications.',
    },
  },
  {
    id: generateId('Token'),
    name: 'Token',
    category: 'Foundational Concepts',
    content: {
      simpleDefinition: 'The fundamental "building block" of text processed and generated by a Large Language Model (LLM), typically part of a word, a whole word, or punctuation.',
      analogy: 'You can think of them like LEGO bricks, which the AI uses as both input and output.',
      example: '`hello` = 1 token; `supercalifragilisticexpialidocious` = 4-5 tokens. (Rule of thumb: 1 token ≈ 4 characters or 0.75 words).',
      elaboration: "LLMs don't process text character by character or word by word in a simple sense. Instead, they break down text into these sub-word units called tokens. This allows them to handle rare words more effectively and manage vocabulary size. The number of tokens directly impacts the computational resources required for processing and generating text, and it's also how usage costs are often calculated for API access to LLMs.",
      whyItMatters: 'Token limits affect AI processing and generation capacity, helping manage input size and anticipate output length, and influencing the cost-efficiency of using LLMs.',
    },
  },
  {
    id: generateId('Prompt'),
    name: 'Prompt',
    category: 'Foundational Concepts',
    content: {
      simpleDefinition: 'The instruction or input given to a Generative AI model to guide its output.',
      example: '"Write a short email to the sales team about Q3 performance."',
      analogy: 'A recipe for a chef; clear instructions lead to better results.',
      elaboration: 'A prompt can be a simple question, a detailed command, or even an example of the desired output. Its effectiveness hinges on its clarity, specificity, and the context it provides. A well-crafted prompt helps steer the AI towards generating relevant, accurate, and high-quality content, acting as the primary interface for human-AI interaction.',
      whyItMatters: "Directly impacts AI output quality; it's how you \"talk\" to the AI and is fundamental to controlling and directing its creative capabilities.",
    },
  },
  {
    id: generateId('Context Window'),
    name: 'Context Window',
    category: 'Foundational Concepts',
    content: {
      simpleDefinition: 'The amount of text (measured in Tokens) that an Large Language Model (LLM) can consider at one time when processing an input or generating a response. It defines the AI\'s "short-term memory" for a given interaction.',
      analogy: "Imagine talking to someone who has a very small notepad. They can remember everything you say as long as it fits on the notepad. Once it's full, they forget the oldest information to make room for new notes. The notepad size is their context window.",
      example: "If an LLM has a 4,000-token context window and you provide a 5,000-token document, the model will only \"see\" and process the most recent 4,000 tokens, ignoring the beginning. Similarly, in a long conversation, it might forget early details depending on how much data is being stored in its Context Window.",
      whyItMatters: "The size of the Context Window significantly impacts an AI's ability to maintain coherence over long conversations, understand lengthy documents, accurately follow multi-step instructions, and grasp complex themes. It's a key practical limitation and development frontier for LLMs.",
    },
  },
  {
    id: generateId('AI Agents'),
    name: 'AI Agents',
    category: 'Foundational Concepts',
    content: {
      simpleDefinition: 'Autonomous AI systems that perceive their environment, process information, make decisions, and take actions to achieve specific goals, often interacting with humans or other systems.',
      analogy: 'Like a personal assistant who not only understands your requests but can also proactively gather information, plan steps, and execute tasks on your behalf without constant supervision.',
      example: 'An AI scheduling assistant that manages your calendar and sends meeting invites, an AI in a game that navigates a virtual world and makes strategic moves, or the Coding Agent that helps an engineer write code in their IDE.',
      whyItMatters: 'Agents represent a significant advancement towards more independent and goal-oriented AI applications, moving beyond simple response generation to autonomous task execution and problem-solving.',
    },
  },
  {
    id: generateId('Latent Space'),
    name: 'Latent Space',
    category: 'Foundational Concepts',
    content: {
      simpleDefinition: 'A high-dimensional space where complex data (like images, text, or audio) is encoded as numerical vectors capturing its essential features.',
      analogy: 'Imagine a massive spice rack where every blend of flavors (data) has its own spot—nearby spices taste similar, distant ones very different.',
      example: 'Word embeddings map "king" and "queen" to nearby points in latent space, reflecting their semantic similarity.',
      whyItMatters: 'Enables machines to compare, interpolate, and generate new data by navigating this continuous representation—crucial for everything from image synthesis to semantic search.',
    },
  },

  // Interaction & Refinement
  {
    id: generateId('Prompt Engineering'),
    name: 'Prompt Engineering',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'The skill of crafting effective Prompts to achieve desired AI results by understanding how the AI "thinks".',
      analogy: "Imagine you're a head chef giving instructions to a very talented cook. If you simply say, \"Make soup,\" you'll get a generic soup. But if you provide a precise, detailed recipe like, \"Make a rich, creamy tomato bisque, slightly smoky, with a basil chiffonade, serving four,\" the cook can deliver that exact, perfect dish. Prompt Engineering is like learning to give those precise, detailed \"recipes\" to your AI to get the exact, tailored content you want.",
      example: 'Asking for a "hyper-realistic photograph of a stoic golden retriever wearing spectacles, sitting in a leather armchair by a fireplace," instead of just "a picture."',
      elaboration: "This discipline involves experimenting with different phrasing, adding constraints, specifying tone, format, and style, and providing examples or context. It's an iterative process of refining inputs to elicit the most accurate, creative, or useful responses from the AI. Effective prompt engineering can significantly improve the utility and reliability of Generative AI applications.",
      whyItMatters: "Crucial for unlocking Generative AI's full potential, ensuring relevant outputs, and impacting work with AI agents by maximizing their effectiveness and minimizing undesirable results.",
    },
    interactiveTools: [
      {
        name: 'Prompt Engineering Guide',
        url: 'https://github.com/anthropics/prompt-eng-interactive-tutorial',
        description: 'An interactive guide on prompt engineering from Anthropic.',
      },
    ],
  },
  {
    id: generateId('Embeddings'),
    name: 'Embeddings',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'Numerical representations capturing the meaning or characteristics of data (words, sentences, images); similar meanings have similar numerical representations.',
      analogy: 'Numerical "fingerprints" for ideas, where similar ideas have numerically close "fingerprints."',
      example: '"King" and "queen" have very similar Embeddings, differing mainly in "gender."',
      elaboration: "These high-dimensional vectors allow AI models to quantify the relationships and similarities between different pieces of information. For instance, in a search engine, the query's embedding can be compared to document embeddings to find the most semantically relevant results, even if the exact keywords aren't present. They are crucial for tasks like semantic search, content recommendation, and understanding contextual relationships.",
      whyItMatters: "Fundamental to how AI understands context, similarity, and performs tasks like search, recommendation, and coherent text generation. They are the AI's internal language for representing and processing meaning.",
    },
    interactiveTools: [
      {
        name: 'Wizmap tool',
        url: 'https://poloclub.github.io/wizmap/',
        description: 'Easy to use tool for visualizing embeddings.',
      },
      {
        name: 'TensorFlow Embeddings Projector',
        url: 'https://projector.tensorflow.org/',
        description: 'Advanced tool for exploring embeddings. Click on cylindrical dots to see closest points (words) and their distance.',
      },
    ],
  },
  {
    id: generateId('Model Parameters'),
    name: 'Model Parameters',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'Internal values or "knobs" within an AI model adjusted during Training Process; more parameters allow learning more complex patterns.',
      analogy: 'Imagine a complex musical instrument like a grand piano. Each string, hammer, and pedal mechanism is precisely tuned and adjusted. These individual adjustments, which collectively determine the instrument\'s sound quality and versatility, are like the AI\'s "parameters." The more finely tuned and numerous these adjustable parts are, the richer and more nuanced the music it can produce.',
      example: 'A small AI model with perhaps a few thousand parameters might be used for a very specific task, like identifying whether a picture contains a cat or a dog. However, a massive Large Language Model (LLM) with billions or even trillions of parameters has learned an immensely complex web of relationships. This allows it to perform highly sophisticated tasks.',
      elaboration: 'These parameters are numerical weights and biases that the model adjusts during its training phase as it learns from the Training Data. They essentially encode the knowledge and patterns the model has learned. A higher number of parameters generally means a more complex model capable of learning more intricate relationships, but also requires more computational power for training and inference, and can be more prone to Hallucination if not properly managed.',
      whyItMatters: "Parameter count often correlates with a model's size and capability, indicating its ability to understand nuances and generate complex outputs. Larger models are usually more powerful but also more computationally expensive and require significant resources for development and deployment.",
    },
  },
  {
    id: generateId('Retrieval Augmented Generation (RAG)'),
    name: 'Retrieval Augmented Generation (RAG)',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'A technique that enhances a **Large Language Model (LLM)** by allowing it to retrieve relevant information from an external, trusted knowledge base (like a database or documents) before generating a response.',
      analogy: 'Imagine a brilliant student who, before answering a question, quickly looks up the most relevant facts in a reliable textbook or research paper. This allows them to give a more accurate, up-to-date, and well-supported answer, rather than just relying on what they vaguely remember.',
      example: 'If you ask an **LLM** about a very recent company policy, instead of potentially **Hallucinating** or giving outdated information, a **RAG** system would first search your company\'s internal knowledge base for the policy, then use that retrieved information to formulate a precise answer. This is crucial for applications like customer support or internal knowledge management.',
      whyItMatters: '**RAG** significantly reduces **Hallucination** in **LLM**s, improves factual accuracy, and allows models to provide answers based on the most current or proprietary information, making them much more reliable for business-critical applications.',
    },
  },
  {
    id: generateId('Model Context Protocol (MCP)'),
    name: 'Model Context Protocol (MCP)',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'A standardized set of rules or an interface that allows an AI model (especially LLMs and AI agents) to effectively connect with and use external tools, functions, or APIs to accomplish tasks.',
      analogy: 'Think of how APIs (Application Programming Interfaces) standardized the way different parts of the internet communicate and share information (e.g., how your weather app gets data from a weather service). The Model Context Protocol (MCP) does something similar for AI, providing a standard "language" for an AI to access and use specialized tools (like a calculator, a calendar, or a database searcher).',
      example: 'Instead of just generating text, an LLM using an MCP server could:\n- **Check the current weather** in a city by calling a weather API.\n- **Book a flight** by interacting with an airline\'s booking system.\n- **Search a company\'s internal product catalog** to answer a customer\'s specific query.\n- **Send an email** via an email service.',
      whyItMatters: 'An effective **Model Context Protocol (MCP)** drastically expands what AI models can do. It allows them to move beyond just conversation or text generation to perform complex, real-world actions, making them much more powerful and versatile tools for automation and problem-solving.',
    },
  },
  {
    id: generateId('Inference'),
    name: 'Inference',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'The process where a trained AI model takes a new input (like a Prompt) and generates an output or makes a prediction. It\'s the "runtime" phase of an AI model.',
      analogy: 'If training is like a student studying for an exam, then inference is like the student actually taking the exam and providing answers based on what they\'ve learned.',
      example: 'When you type a question into an AI chatbot and it gives you a response, that\'s the LLM performing inference. When an image generator creates a picture from your text, that\'s also inference.',
      whyItMatters: "Inference is the practical application of a trained AI model; it's how AI delivers its value by processing real-world inputs and generating useful outputs in real-time.",
    },
  },
  {
    id: generateId('Training Data'),
    name: 'Training Data',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'The vast information (text, images, code) an AI model learns from during initial development, forming its knowledge and abilities.',
      analogy: "A student's entire library and archives, from which they study.",
      example: 'Billions of web pages, books, and articles for an LLM.',
      elaboration: 'The quality, quantity, and diversity of this data are paramount. If the Training Data is biased, incomplete, or contains errors, the AI model will inherit and potentially amplify those issues, leading to biased or inaccurate outputs. Curating clean, representative, and ethical training datasets is a massive and ongoing effort in AI development.',
      whyItMatters: 'Quality, quantity, and diversity of Training Data directly determine AI capabilities and potential Bias. Crucial for Data Governance and understanding AI limitations, as it dictates what the AI knows and how it behaves.',
    },
  },
  {
    id: generateId('Fine-tuning'),
    name: 'Fine-tuning',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'Further training an already developed Generative AI model on a smaller, specific dataset to adapt it for a particular task or domain.',
      analogy: 'A general medical doctor (the pre-trained model) who then specializes in cardiology (fine-tuning) by studying more specific heart-related cases. They now know more deeply about a niche.',
      example: 'Taking a general **LLM** and fine-tuning it with a legal firm\'s historical case documents and legal briefs so it can accurately summarize specific types of legal precedents or draft initial responses to common legal inquiries.',
      elaboration: 'This process takes a pre-trained "base" model and exposes it to a smaller, more specialized dataset relevant to a particular use case. This allows the model to learn specific styles, terminology, or factual knowledge pertinent to that domain without having to train a new model from scratch, which is computationally expensive. It can significantly improve performance on niche tasks.',
      whyItMatters: 'Fine-tuning allows companies to tailor powerful general AI models for their specific needs, making them much more useful for internal applications without having to build a model from scratch.',
    },
  },
  {
    id: generateId('Reinforcement Learning from Human Feedback (RLHF)'),
    name: 'Reinforcement Learning from Human Feedback (RLHF)',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'Humans rate Generative AI outputs, and this feedback further trains the AI to produce more helpful, accurate, and aligned responses.',
      analogy: 'Teaching a dog tricks with rewards and corrections to guide their behavior.',
      example: 'Humans selecting the best of three AI-generated answers, teaching the AI to produce more like it.',
      elaboration: 'In RLHF, human evaluators provide preferences for different AI-generated outputs. This feedback is then used to train a "reward model," which in turn guides the primary Generative AI model to produce outputs that are more aligned with human values and instructions. This iterative process helps steer the AI away from undesirable behaviors (like Hallucination or generating harmful content) and towards more helpful and safe interactions.',
      whyItMatters: 'A key reason modern LLMs are conversational and aligned with human intentions, making them safer and more useful by incorporating human preferences directly into the training loop.',
    },
  },
  {
    id: generateId('Hallucination'),
    name: 'Hallucination',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'When a Generative AI model confidently produces false, nonsensical, or unsubstantiated information. It "makes things up."',
      analogy: 'A confident person giving incorrect information.',
      example: 'An LLM providing a perfectly formatted but non-existent academic paper citation.',
      elaboration: 'Hallucinations occur because LLMs are essentially sophisticated pattern-matching systems. They generate text by predicting the most probable next token based on their Training Data. If the training data contains inconsistencies, or if the model encounters a prompt outside its learned patterns, it might generate plausible-sounding but factually incorrect information. This is a significant challenge for reliability and trust in AI outputs.',
      whyItMatters: 'A critical challenge requiring fact-checking of AI outputs. Highlights the need for human oversight and validation in Data Governance and emphasizes that AI outputs should not be blindly trusted, especially for critical information.',
    },
  },

  {
    id: generateId('Instruction Tuning'),
    name: 'Instruction Tuning',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'The process of fine-tuning a pre-trained model on paired examples of instructions and ideal responses to make it better at following user commands.',
      analogy: 'Like a coach drilling an athlete on specific plays, ensuring they respond perfectly when the game clock starts.',
      example: 'InstructGPT was tuned on thousands of "Write X in style Y" examples so it learns to reliably obey diverse prompts.',
      whyItMatters: 'Significantly improves usability and safety by aligning model outputs with user intent and reducing undesirable behaviors.',
    },
    interactiveTools: [
      {
        name: 'Llama 3.2 3B Instruct Demo',
        url: 'https://huggingface.co/spaces/huggingface-projects/llama-3.2-3B-Instruct',
        description: 'Interact with Meta\'s instruction-tuned Llama 3.2 3B model directly in your browser—no signup or installation required.\nIn the "Prompt" field, type your instruction and any input text (e.g., "Summarize: <your text>"), then click "Generate" to see the model follow your command.\nUse the "Examples" tab to load pre-built instruction templates or enter your own custom instructions to explore how the model responds to diverse tasks.',
      },
    ],
  },
  {
    id: generateId('Chain of Thought (CoT)'),
    name: 'Chain of Thought (CoT)',
    category: 'Interaction & Refinement',
    content: {
      simpleDefinition: 'A prompting technique that asks a model to generate its intermediate reasoning steps before giving a final answer.',
      analogy: 'Showing your scratch work in math class so the teacher can follow your logic, not just the answer.',
      example: 'Prompting "Let\'s think step by step" often yields more accurate arithmetic or logical reasoning from GPT-style models.',
      whyItMatters: 'Dramatically boosts performance on complex tasks by making the model\'s latent reasoning explicit and less error-prone.',
    },
  },

  // AI Architectures & Capabilities
  {
    id: generateId('Transformer Architecture'),
    name: 'Transformer Architecture',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'A neural network design enabling AI to process and generate sequences (like text) by efficiently "paying attention" to relevant parts of the input, regardless of distance.',
      analogy: 'Imagine you\'re trying to understand a complex conversation in a crowded room. Your brain doesn\'t just focus on the last few words; it constantly connects what\'s being said *now* to things said minutes ago, who said what, and even subtle cues. You "pay attention" to all relevant pieces of information, no matter how far apart they are in the conversation. The Transformer does this for words in a sentence.',
      example: 'Before Transformers, if you gave an AI a very long sentence or document, it might struggle to understand how words at the beginning related to words at the end, leading to awkward translations or summaries. Transformers allow the AI to "see" and weigh the importance of *all* words simultaneously. This is why modern AI can accurately translate entire paragraphs, write coherent long-form articles, or summarize complex reports, because it understands the deep connections across the text.',
      elaboration: 'The core innovation of the Transformer is its "self-attention" mechanism. This mechanism allows the model to weigh the importance of all other words in a sentence when processing a single word, capturing long-range dependencies and contextual relationships far more effectively than previous architectures. This parallel processing capability also makes Transformers highly efficient for training on large datasets.',
      whyItMatters: 'This architecture is the core technological innovation behind almost all modern Large Language Model (LLM)s and advanced Generative AI systems, enabling their remarkable abilities in understanding and generating complex human language.',
    },
    interactiveTools: [
      {
        name: 'Transformer Explainer',
        url: 'https://poloclub.github.io/transformer-explainer/',
        description: 'A tool by Polo Club of Data Science, Georgia Tech, to explain Transformer models. May not be high level enough for true beginners.',
      },
    ],
  },
  {
    id: generateId('Reasoning Models'),
    name: 'Reasoning Models',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'AI models designed to "think" step-by-step, performing logical inference, planning, and problem-solving, rather than simply predicting the next token based on probability.',
      analogy: 'Think of a human detective meticulously piecing together clues, drawing conclusions, and forming a logical chain of events, rather than just guessing the outcome. They follow a process of deduction.',
      example: 'A **Reasoning Model** could be given a complex medical case with symptoms, test results, and patient history, and it would systematically analyze the information, consider possible diagnoses, and propose a treatment plan, explaining its logical steps. This differs from an **LLM** that might just generate a plausible-sounding diagnosis without true step-by-step logical processing.',
      elaboration: 'While many **Large Language Model (LLM)**s excel at generating fluent and coherent text, their core mechanism often relies on predicting the most probable next word or phrase. **Reasoning Models**, however, are designed with additional components or training specifically to perform explicit chains of thought, break down problems into sub-problems, and apply logical rules or symbolic manipulation. This allows them to arrive at solutions through a verifiable sequence of steps, making their conclusions more robust and interpretable, particularly for tasks requiring mathematical precision, strategic planning, or deep causal understanding.',
      whyItMatters: '**Reasoning Models** are crucial for AI to tackle complex, multi-step problems that require true understanding and logical deduction, moving beyond pattern matching to more robust and verifiable problem-solving in areas like scientific discovery, strategic planning, or complex diagnostics.',
    },
  },
  {
    id: generateId('Diffusion Models'),
    name: 'Diffusion Models',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'A powerful Generative AI model that creates new data (especially images and more recently with text) by gradually reversing a noise-adding process, "denoising" from pure static to a clear image.',
      analogy: 'Clarifying a blurry, static-filled TV screen into a perfect new image.',
      example: 'Generating photorealistic images from Prompts (Midjourney, DALL-E 2).',
      elaboration: 'Diffusion models operate in two phases. The "forward diffusion" process involves gradually adding Gaussian noise to an image over many steps until it becomes pure random noise. During training, the model learns to predict and reverse each of these noise-adding steps. The "reverse diffusion" process is the generative part: starting from pure noise, the model iteratively removes noise, guided by what it learned in the forward process, to progressively transform the static into a coherent, high-quality, and *novel* image. Both processes are crucial because the model learns *how* to generate by understanding *how* data degrades into noise.',
      whyItMatters: 'Currently state-of-the-art for image/video generation, driving visual creativity in design, art, and entertainment, and enabling new applications in content creation and digital media.',
    },
    interactiveTools: [
       {
        name: 'Diffusion Explainer',
        url: 'https://poloclub.github.io/diffusion-explainer/',
        description: 'Launch the Diffusion Explainer demo in your browser—no signup, code, or GPU needed. Select a sample prompt or enter your own, then scrub through timesteps to see how the image is progressively refined. Click components like "Text Representation Generator" or "Image Representation Refiner" to toggle between architecture overview and detailed visualizations of each stage.',
      },
    ],
  },
  {
    id: generateId('Multimodal Models'),
    name: 'Multimodal Models',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'AI systems capable of processing and generating content across multiple types of data simultaneously, such as text, images, audio, and video.',
      analogy: 'Like a human who can understand a conversation by listening, watching body language, and reading facial expressions all at once, and then respond using voice, gestures, or writing.',
      example: 'An AI that can generate a video from a text description, create a text caption for an image, or describe an image using both visual input and audio cues.',
      whyItMatters: 'Multimodal AI represents a significant leap towards more human-like AI understanding and interaction, enabling richer applications that blend different forms of information for more comprehensive tasks.',
    },
  },
  {
    id: generateId('Neural Network'),
    name: 'Neural Network',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'A computational model inspired by the structure and function of the human brain, consisting of interconnected "nodes" or "neurons" organized in layers, designed to recognize patterns and learn from data.',
      analogy: 'Think of it like a team of interconnected specialists. Each specialist (neuron) processes a small piece of information and passes it on, and by working together, the whole team can solve complex problems.',
      example: 'Used in image recognition (identifying faces in photos), speech recognition (voice assistants), and the core of LLMs like the Transformer Architecture.',
      whyItMatters: 'Neural Networks are the fundamental architecture underpinning most advanced AI, including all Generative AI models, enabling them to learn complex patterns and perform sophisticated tasks.',
    },
    interactiveTools: [
      {
        name: 'TensorFlow Playground',
        url: 'https://playground.tensorflow.org/',
        description: 'Explore how neural networks learn right in your browser—no coding required. Use sliders to adjust learning rate, layers, and activation functions. Click "Play" to train a model live on colorful 2D datasets and see how decision boundaries evolve in real time. Ideal for intuitively grasping core neural net concepts.',
      },
    ],
  },
  {
    id: generateId('Generative Adversarial Network (GAN)'),
    name: 'Generative Adversarial Network (GAN)',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'A Generative AI model with two competing neural networks: a "Generator" creating fakes, and a "Discriminator" identifying them, improving through competition.',
      analogy: 'Imagine a **chef** (the Generator) who is trying to create a new, delicious dessert recipe. They keep trying different ingredient combinations and techniques. At the same time, there\'s a **food critic** (the Discriminator) whose job is to taste desserts and decide if they are truly innovative and well-made, or just mediocre. The chef learns to improve their recipes by getting feedback from the critic (when their desserts are rated poorly). The critic, in turn, gets better at discerning quality by tasting many desserts from both the innovative chef and other average cooks. This constant, competitive feedback loop makes the chef\'s creations increasingly impressive and the critic\'s palate increasingly refined.',
      example: 'GANs are famously used to create highly realistic synthetic data. For example, they can generate:\n- **Photorealistic images of human faces that do not belong to any real person.** These are often used in stock photography or as profile pictures for fictional identities.\n- **New product designs or architectural blueprints** that blend existing styles in novel ways.\n- **Synthetic medical images** for training diagnostic AI without using sensitive patient data.',
      elaboration: 'The Generator network\'s goal is to produce data (e.g., images) that are indistinguishable from real data. The Discriminator network\'s job is to distinguish between real data and the Generator\'s fakes. As they train, the Generator gets better at creating convincing fakes, and the Discriminator gets better at spotting them. This adversarial process drives both networks to improve, leading to highly realistic generated content.',
      whyItMatters: 'Groundbreaking for realistic content generation; understanding GANs helps appreciate Generative AI evolution, even as Diffusion Models dominate. They paved the way for many advanced generative techniques.',
    },
  },
  {
    id: generateId('Machine Learning (ML)'),
    name: 'Machine Learning (ML)',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'A subset of Artificial Intelligence (AI) that focuses on enabling systems to learn from data, identify patterns, and make decisions with minimal human intervention.',
      analogy: 'Imagine teaching a child to recognize different animals by showing them many pictures, rather than giving them a fixed set of rules. ML is how computers learn from "experience" (data).',
      example: 'Recommendation engines (like Netflix suggestions), fraud detection systems, and predictive analytics in business are all powered by ML.',
      whyItMatters: 'ML is the primary method by which modern AI systems, including all Generative AI models, acquire their intelligence and adapt to new information.',
    },
  },
  {
    id: generateId('Self-Attention / Attention Mechanism'),
    name: 'Self-Attention / Attention Mechanism',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'A network component that lets a model weigh the importance of each input element relative to others when producing an output.',
      analogy: 'Like a group discussion where you tune in louder to the most relevant speaker rather than giving everyone equal airtime.',
      example: 'In a Transformer, self-attention helps the model link "bank" in "river bank" to "river" rather than "financial institution."',
      whyItMatters: 'Powers state-of-the-art models by enabling context-aware processing and parallel computation—transforming tasks in translation, summarization, and more.',
    },
    interactiveTools: [
      {
        name: 'Comet Explainable AI for Transformers',
        url: 'https://www.comet.com/site/blog/explainable-ai-for-transformers',
        description: 'Explore attention patterns directly in-browser—no setup required. The embedded Comet panel loads a demo model pre-configured with attention data.\nSimply open the "Transformers Attention Head Viewer" panel, then click any layer and head combination to visualize token-to-token attention heatmaps in real time.',
      },
    ],
  },
  {
    id: generateId('LoRA (Low-Rank Adaptation)'),
    name: 'LoRA (Low-Rank Adaptation)',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'A parameter-efficient fine-tuning method that injects small, trainable low-rank matrices into a frozen pre-trained model.',
      analogy: 'Like adding lightweight expansion cards to your computer instead of replacing the entire motherboard.',
      example: 'Applying LoRA to LLaMA lets you specialize for a new domain with only a fraction of the training cost and storage.',
      whyItMatters: 'Makes fine-tuning large models accessible on limited hardware and speeds up iteration—key for rapid prototyping and edge deployments.',
    },
    interactiveTools: [
      {
        name: 'Lora The Explorer',
        url: 'https://huggingface.co/spaces/multimodalart/LoraTheExplorer',
        description: 'Experiment with LoRA fine-tuning by generating images with different style adapters. Choose a LoRA model, enter a prompt (e.g., "A photo of a futuristic vehicle"), and click "Generate." This lets you visually explore how LoRA changes output using lightweight parameter injection—directly in-browser with no signup or install required.',
      },
    ],
  },
  {
    id: generateId('Zero-Shot / Few-Shot / One-Shot Learning'),
    name: 'Zero-Shot / Few-Shot / One-Shot Learning',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'Techniques where a model performs a task with zero, one, or only a handful of examples provided at inference time.',
      analogy: 'Meeting someone once (one-shot) or reading a brief description (few-shot) and immediately understanding what to do.',
      example: 'GPT-3 can translate between languages "zero-shot" by simply being asked in English: "Translate into French: \'Good morning.\'"',
      whyItMatters: 'Offers tremendous flexibility—no re-training required for every new task, accelerating deployment across varied use cases.',
    },
    interactiveTools: [
      {
        name: 'Xenova Zero-Shot Classifier',
        url: 'https://mj106-xformjs.static.hf.space/zero-shot-classification.html',
        description: 'Try out zero-shot learning by entering a sentence and a list of labels—no signup needed. For example, paste:\n"The stock market rallied after a favorable jobs report"\nwith labels like: "economy, sports, entertainment"\nThen click "Classify" to see how the model predicts the most relevant label without ever being trained on that specific task. You can modify the sentence or labels to explore how well it generalizes.',
      },
    ],
  },
  {
    id: generateId('Vector Database'),
    name: 'Vector Database',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'A specialized database optimized for storing and retrieving high-dimensional vectors based on similarity search.',
      analogy: 'Like a library organized not by title or author but by the content\'s thematic similarity—pulling related books even if they don\'t share obvious keywords.',
      example: 'Pinecone or Weaviate serve as backends for retrieval-augmented generation, fetching relevant passages given an embedding query.',
      whyItMatters: 'Underpins semantic search, recommendation engines, and chatbots that need lightning-fast access to contextually related information.',
    },
  },
  {
    id: generateId('Temperature / Top-p'),
    name: 'Temperature / Top-p',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'Hyperparameters controlling the randomness of a model\'s next-token selection—temperature adjusts distribution "sharpness," top-p limits sampling to the most probable tokens.',
      analogy: 'Temperature is the spice dial (higher = more adventurous); top-p is the tasting spoon that only picks from your top-favorite ingredients.',
      example: 'Setting temperature=0.2 yields very conservative text; top-p=0.9 lets the model choose from the top 90% probable words, balancing creativity and coherence.',
      whyItMatters: 'Fine-tuning these settings lets you steer outputs toward formulaic precision or creative variety, depending on your application\'s needs.',
    },
    interactiveTools: [
      {
        name: 'OpenRouter Playground',
        url: 'https://openrouter.ai/chat',
        description: 'Use OpenRouter\'s chat playground to see how temperature and top-p influence a model\'s creativity. Log in, choose a model, then open "Settings" to adjust temperature (controls randomness) and top-p (limits sampling to top-probable tokens). Try the same prompt with different values to observe how outputs shift between predictable and imaginative responses.',
      },
    ],
  },
  {
    id: generateId('GAN Dissection (GANPaint)'),
    name: 'GAN Dissection (GANPaint)',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'A tool for exploring the internal neurons of a GAN, showing how specific units correspond to interpretable concepts like "tree," "door," or "tower."',
      analogy: 'It\'s like a brain surgery toolkit for GANs—you can toggle on/off specific "neurons" and see how the sample image changes in real time.',
      example: 'In GANPaint, click "tree" to activate tree-related units—then add or remove trees from a generated scene. Flip "door" units to insert or remove a door.',
      whyItMatters: 'Demonstrates that GANs learn compositional, object-level representations internally. Enables causal manipulation of generated content and improves interpretability in generative models.',
    },
    interactiveTools: [
      {
        name: 'GANPaint / GAN Dissection Demo',
        url: 'https://gandissect.csail.mit.edu/',
        description: 'Explore which neurons control what in a GAN. Launch the demo, select objects like "trees" or "doors," and flip their neurons on/off to add/remove them from generated scenes. Observe how GANs build compositions atop these internal units—all directly in-browser and no code required.',
      },
    ],
  },
  {
    id: generateId('GAN Lab'),
    name: 'GAN Lab',
    category: 'AI Architectures & Capabilities',
    content: {
      simpleDefinition: 'An interactive in-browser tool that helps users visualize and train simple GANs (Generator & Discriminator) step-by-step on toy 2D distributions.',
      analogy: 'Think of two sculptors working together—one creates shapes, the other critiques them—and they iterate until the shapes look real.',
      example: 'Use GAN Lab to train on sample data like blended circles or spirals. You can pulse through each iteration or slow-motion to watch how generated points increasingly match the real data.',
      whyItMatters: 'Makes the adversarial training process tangible, revealing how both networks evolve together and offering intuitive insight into GAN dynamics.',
    },
    interactiveTools: [
      {
        name: 'GAN Lab',
        url: 'https://poloclub.github.io/ganlab/',
        description: 'Run GAN Lab directly in your browser—no installation needed. Start with sample distributions or draw your own. Use play, slow-motion, or step controls to watch generator and discriminator updates unfold. Adjust hyperparameters live (learning rate, batch size, noise) to see their effect on training dynamics and model performance.',
      },
    ],
  },

  // Future & Research Landscape
  {
    id: generateId('AGI (Artificial General Intelligence)'),
    name: 'AGI (Artificial General Intelligence)',
    category: 'Future & Research Landscape',
    content: {
      simpleDefinition: 'AI with **human-level intelligence** across all intellectual tasks, capable of understanding, learning, and applying intelligence broadly.',
      analogy: 'A true "thinking machine" like C-3PO, performing diverse tasks and understanding human nuances.',
      example: 'An AGI could write poems, negotiate contracts, and perform surgery.',
      elaboration: 'AGI would not be limited to a specific domain (like playing chess or generating text). It would possess cognitive abilities comparable to a human, including common sense, creativity, and the ability to learn from experience across various fields. Achieving AGI is a monumental challenge and a subject of intense debate.',
      whyItMatters: 'AGI represents a major AI research goal with profound societal implications, including for Data Governance and AI integration, as it will fundamentally change the nature of work and human-technology interaction.',
    },
  },
  {
    id: generateId('ASI (Artificial Super Intelligence)'),
    name: 'ASI (Artificial Super Intelligence)',
    category: 'Future & Research Landscape',
    content: {
      simpleDefinition: 'AI **far surpassing human intelligence** in all fields, including creativity, wisdom, and social skills.',
      analogy: 'A mind beyond human comprehension, solving problems humans haven\'t conceived.',
      example: 'An ASI could develop new scientific theories in minutes or solve global challenges with ease.',
      elaboration: 'ASI is a concept that goes beyond merely matching human intelligence; it posits an intelligence that is orders of magnitude greater. This could lead to an "[intelligence explosion](https://www.forethought.org/research/three-types-of-intelligence-explosion)" where the ASI rapidly improves itself, creating an even more powerful intelligence. This concept raises significant ethical and control challenges, often debated in the context of existential risk and the future of humanity.',
      whyItMatters: 'This concept is moving from theoretical to a question of when do we get ASI, it drives discussions on future ethics and control of advanced AI, highlighting responsible AI development and the need for robust safeguards as AI capabilities continue to advance.',
    },
  },
  {
    id: generateId('Tree of Thought'),
    name: 'Tree of Thought',
    category: 'Future & Research Landscape',
    content: {
      simpleDefinition: 'An emerging approach that explores multiple parallel chains of thought (branches), evaluates them, and backtracks to the most promising paths.',
      analogy: 'Like navigating a maze by sending out multiple explorers down different corridors, then picking the one that leads closest to the exit.',
      example: 'Research prototypes generate and score several reasoning paths for a puzzle before committing to the final solution.',
      whyItMatters: 'Promises further gains in reasoning quality by allowing models to recover from dead-end thoughts and explore richer solution spaces.',
    },
    interactiveTools: [
      {
        name: 'Tree of Thought GitHub Demo',
        url: 'https://github.com/princeton-nlp/tree-of-thought-llm',
        description: 'Explore Tree of Thought reasoning by cloning the original research repo from Princeton NLP. The included Python notebooks let you define problems, generate multiple reasoning paths, and apply search strategies like BFS or self-consistency. Best used by launching in Colab or your preferred notebook environment.',
      },
    ],
  },
];

export const categories = [
  'Foundational Concepts',
  'Interaction & Refinement',
  'AI Architectures & Capabilities',
  'Future & Research Landscape'
];
